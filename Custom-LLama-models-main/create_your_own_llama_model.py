# -*- coding: utf-8 -*-
"""Create Your Own Llama Model

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1L7Hs1lWuqvNUUH7kjAmjeTC8RfOmNXQu
"""

# COLAB / LOCAL: Install dependencies (run in a notebook cell or shell)
!pip install -q "transformers>=4.30.0" datasets tokenizers "accelerate>=0.20.0" bitsandbytes sentencepiece safetensors huggingface_hub

!pip install -q "datasets" "tokenizers" "transformers" "accelerate" "safetensors"

!pip install -q datasets tokenizers transformers accelerate safetensors

import torch
from datasets import load_dataset
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors
from transformers import (PreTrainedTokenizerFast,
                          GPT2Config, GPT2LMHeadModel,
                          DataCollatorForLanguageModeling,
                          TrainingArguments, Trainer)

# 1) Load WikiText-2 (works out-of-the-box)
#smaller dataset
ds = load_dataset("wikitext", "wikitext-2-raw-v1", split="train")
print("Dataset size (lines):", len(ds))

# 2) Train a BPE(Byte Pair Encoding) tokenizer on a subset of Wiki lines
def train_tokenizer(dataset, output_path="tokenizer.json", vocab_size=20000, sample_size=10000):
    def iterator():
        for i, ex in enumerate(dataset):
            text = ex.get("text") or ""
            if text.strip():
                yield text
            if i >= sample_size - 1:
                break

    tokenizer = Tokenizer(models.BPE(unk_token="<unk>"))
    tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel()
    trainer = trainers.BpeTrainer(vocab_size=vocab_size,
                                  special_tokens=["<pad>", "<bos>", "<eos>", "<unk>"])
    tokenizer.train_from_iterator(iterator(), trainer=trainer)
    tokenizer.post_processor = processors.ByteLevel(trim_offsets=True)
    tokenizer.save(output_path)
    return output_path

tk_path = train_tokenizer(ds, output_path="tokenizer.json")
print(f"Tokenizer saved to {tk_path}")

# Wraps our custom tokenizer into a Transformers-compatible object
tokenizer = PreTrainedTokenizerFast(
    tokenizer_file="tokenizer.json",
    unk_token="<unk>", bos_token="<bos>",
    eos_token="<eos>", pad_token="<pad>"
)
print("Vocab size:", tokenizer.vocab_size)

# 3) Tokenize and group into blocks
block_size = 256

#Tokenizes dataset into token IDs
def tokenize_fn(examples):
    return tokenizer(examples["text"], truncation=False)
#Removes "text" column so only input_ids remain
tokenized = ds.map(tokenize_fn, batched=True, remove_columns=ds.column_names)

def group_texts(examples):
    all_ids = sum(examples["input_ids"], []) #Concatenates all tokens into one list
    total_len = (len(all_ids) // block_size) * block_size
    #Cuts them into non-overlapping chunks of block_size
    blocks = [all_ids[i : i + block_size] for i in range(0, total_len, block_size)]
    #Labels = same as inputs (because we’re predicting the next token)
    return {"input_ids": blocks, "labels": blocks.copy()}

lm_dataset = tokenized.map(group_texts, batched=True, remove_columns=tokenized.column_names) #Creates dataset ready for model training
print("Number of training blocks:", len(lm_dataset))

# 4) Creates a tiny GPT-2: Embedding size = 384 transformer layers attention heads Sequence length = 256 tokens
#Much smaller than real GPT-2 for faster training

config = GPT2Config(
    vocab_size=tokenizer.vocab_size,
    n_positions=block_size,
    n_ctx=block_size,
    n_embd=384,
    n_layer=4,
    n_head=6,
)
model = GPT2LMHeadModel(config)

# 5) Training setup. Prepares batches for causal LM (not masked LM)
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False) #mlm=False means predict next token, not fill in blanks

#Training hyperparameters: Batch size = 2 (very small to fit in memory) Accumulate gradients over 8 steps (effective batch = 16)
#Mixed precision (fp16) if GPU available
#Train for 5 epochs
#Save model after each epoch

training_args = TrainingArguments(
    output_dir="./mini_llama_wikitext2",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    fp16=torch.cuda.is_available(),
    num_train_epochs=5,
    logging_steps=100,
    save_strategy="epoch",
    push_to_hub=False,
)

#Trainer handles batching, optimization, checkpointing
#Actually trains your GPT-like model on WikiText-2
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer,
)

trainer.train()

# 6) Generate sample. Tokenizes prompt and moves to GPU/CPU
#Uses sampling (top_k, top_p, temperature) to make text creative
#Decodes generated tokens to readable text

prompt = "In a distant future,"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
gen = model.generate(
    **inputs,
    max_new_tokens=100,
    do_sample=True,
    top_k=50,
    top_p=0.95,
    temperature=0.9,
)
print("Generated:", tokenizer.decode(gen[0], skip_special_tokens=True))

tokenizer.save_pretrained("./my_model")
model.save_pretrained("./my_model")

from huggingface_hub import login

# This will ask for your token (from https://huggingface.co/settings/tokens)
login()

from huggingface_hub import login, Repository
import shutil

# 1️⃣ Login to Hugging Face
login(token="")  # Replace with your token

# 2️⃣ Clone your model repo
repo = Repository(
    local_dir="my_model_repo",
    clone_from="RadhaShyam/my-mini-llama"  # Replace with your repo ID
)

# 3️⃣ Copy model + tokenizer files to the repo folder
files_to_copy = [
    "config.json",
    "generation_config.json",
    "model.safetensors",          # <-- updated from pytorch_model.bin
    "tokenizer.json",
    "tokenizer_config.json",
    "special_tokens_map.json"
]

for file_name in files_to_copy:
    shutil.copy(f"my_model/{file_name}", "my_model_repo/")

# 4️⃣ Push to Hugging Face Hub
repo.push_to_hub(commit_message="Initial model upload")

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load model from HF Hub
tokenizer = AutoTokenizer.from_pretrained("RadhaShyam/my-mini-llama")
model = AutoModelForCausalLM.from_pretrained("RadhaShyam/my-mini-llama")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)

# Fix missing pad token if needed
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token
if model.config.pad_token_id is None:
    model.config.pad_token_id = tokenizer.eos_token_id

print("Type 'quit' to stop chatting.\n")

while True:
    user_input = input("You: ")
    if user_input.lower() == "quit":
        break

    # Chat prompt template
    prompt = f"User: {user_input}\nAssistant:"

    # Encode
    inputs = tokenizer(prompt, return_tensors="pt", padding=True).to(device)

    # Generate
    outputs = model.generate(
        **inputs,
        max_length=200,
        temperature=0.7,
        top_p=0.9,
        do_sample=True,
        pad_token_id=tokenizer.pad_token_id,
        eos_token_id=tokenizer.eos_token_id
    )

    # Decode & clean
    text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    reply = text.split("Assistant:")[-1].strip()

    print(f"Bot: {reply}\n")

!rm -rf /root/.cache/huggingface

!rm -rf /content/*

#large datase (required 70 GB+ Space)

import torch
from datasets import load_dataset
from tokenizers import Tokenizer, models, trainers, pre_tokenizers, processors
from transformers import (
    PreTrainedTokenizerFast, GPT2Config, GPT2LMHeadModel,
    DataCollatorForLanguageModeling, TrainingArguments, Trainer
)

# 1) Load a valid, available subset — e.g., Pile-CC
ds = load_dataset("ArmelR/the-pile-splitted", "Pile-CC", split="train")
print("Dataset size (lines):", len(ds))

# 2) Train tokenizer on a subset
def train_tokenizer(dataset, output_path="tokenizer.json", vocab_size=20000, sample_size=20000):
    def iterator():
        for i, ex in enumerate(dataset):
            text = ex.get("text") or ""
            if text.strip():
                yield text
            if i >= sample_size - 1:
                break

    tok = Tokenizer(models.BPE(unk_token="<unk>"))
    tok.pre_tokenizer = pre_tokenizers.ByteLevel()
    trainer = trainers.BpeTrainer(vocab_size=vocab_size,
                                  special_tokens=["<pad>", "<bos>", "<eos>", "<unk>"])
    tok.train_from_iterator(iterator(), trainer=trainer)
    tok.post_processor = processors.ByteLevel(trim_offsets=True)
    tok.save(output_path)
    return output_path

tk_path = train_tokenizer(ds, output_path="tokenizer.json")
print("Tokenizer saved to:", tk_path)

tokenizer = PreTrainedTokenizerFast(
    tokenizer_file="tokenizer.json",
    unk_token="<unk>", bos_token="<bos>",
    eos_token="<eos>", pad_token="<pad>"
)
print("Vocab Size:", tokenizer.vocab_size)

# 3) Tokenize and group into blocks
block_size = 256

def tokenize_fn(examples):
    return tokenizer(examples["text"], truncation=False)

tokenized = ds.map(tokenize_fn, batched=True, remove_columns=ds.column_names)

def group_texts(examples):
    all_ids = sum(examples["input_ids"], [])
    total_len = (len(all_ids) // block_size) * block_size
    blocks = [all_ids[i:i + block_size] for i in range(0, total_len, block_size)]
    return {"input_ids": blocks, "labels": blocks.copy()}

lm_dataset = tokenized.map(group_texts, batched=True, remove_columns=tokenized.column_names)
print("Number of training blocks:", len(lm_dataset))

# 4) Build a small GPT-style model
config = GPT2Config(
    vocab_size=tokenizer.vocab_size,
    n_positions=block_size, n_ctx=block_size,
    n_embd=384, n_layer=4, n_head=6
)
model = GPT2LMHeadModel(config)

# 5) Training setup
data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)
training_args = TrainingArguments(
    output_dir="./mini_llama_pilecc",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    fp16=torch.cuda.is_available(),
    num_train_epochs=3,
    logging_steps=100,
    save_strategy="epoch",
    push_to_hub=False,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=lm_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer
)

trainer.train()

# 6) Generate a sample
prompt = "In a distant future,"
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
gen = model.generate(
    **inputs,
    max_new_tokens=100,
    do_sample=True,
    top_k=50, top_p=0.95, temperature=0.9,
)
print("Generated:", tokenizer.decode(gen[0], skip_special_tokens=True))